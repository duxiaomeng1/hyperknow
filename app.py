"""
Gradio ç•Œé¢ï¼šå±•ç¤º Director Agent çš„å®Œæ•´å†³ç­–è¿‡ç¨‹
åŒ…å«ç”¨æˆ·è¯·æ±‚ã€å‡½æ•°è°ƒç”¨è·¯å¾„å’Œæœ€ç»ˆå›å¤
"""
import os
import sys
import json
import gradio as gr
from google import genai
from google.genai import types
from tools.memory_tool import MemoryTool
from tools.select_file_tool import SelectFileTool
from tools.response_generator_tool import ResponseGeneratorTool


# ä» main.py å¯¼å…¥å¿…è¦çš„å‡½æ•°å®šä¹‰
def load_metadata(metadata_path: str = "metadata.json") -> dict:
    """åŠ è½½ metadata.json æ–‡ä»¶"""
    try:
        with open(metadata_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"è­¦å‘Š: æ‰¾ä¸åˆ°æ–‡ä»¶ {metadata_path}ï¼Œä½¿ç”¨ç©ºæ•°æ®")
        return {"files": []}
    except json.JSONDecodeError:
        print(f"è­¦å‘Š: {metadata_path} ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ–‡ä»¶ï¼Œä½¿ç”¨ç©ºæ•°æ®")
        return {"files": []}


def create_select_files_function(metadata_path: str = "metadata.json") -> dict:
    """åŠ¨æ€åˆ›å»ºæ–‡ä»¶é€‰æ‹©å‡½æ•°å£°æ˜"""
    metadata = load_metadata(metadata_path)
    files = metadata.get("files", [])
    
    file_titles = [file_info["title"] for file_info in files]
    
    # ç”Ÿæˆæ–‡ä»¶æè¿°åˆ—è¡¨ï¼ˆåŒ…å«å®Œæ•´æ‘˜è¦ä»¥ä¾¿æ›´å¥½åœ°åŒ¹é…ï¼‰
    file_descriptions = []
    for file_info in files:
        title = file_info["title"]
        summary = file_info.get("content_summary", "æ— æ‘˜è¦")
        # ä¿ç•™å®Œæ•´æ‘˜è¦ï¼Œè®©æ¨¡å‹èƒ½æ›´å‡†ç¡®åœ°é€‰æ‹©
        file_descriptions.append(f"- **{title}**\n  å†…å®¹: {summary}")
    
    # æ„å»ºæè¿°æ–‡æœ¬
    description = f"""æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œéœ€æ±‚ï¼Œä»å¯ç”¨çš„æ–‡æ¡£åº“ä¸­**ç²¾ç¡®é€‰æ‹©**æœ€ç›¸å…³çš„æ–‡ä»¶ã€‚

âš ï¸ **é‡è¦æç¤º**ï¼š
- **ä»”ç»†é˜…è¯»**æ¯ä¸ªæ–‡ä»¶çš„å†…å®¹æ‘˜è¦
- **åªé€‰æ‹©**ä¸ç”¨æˆ·é—®é¢˜**ç›´æ¥ç›¸å…³**çš„æ–‡ä»¶
- **ä¸è¦**é€‰æ‹©æ‰€æœ‰æ–‡ä»¶ï¼Œè¦æ ¹æ®é—®é¢˜å†…å®¹**ç²¾ç¡®åŒ¹é…**
- å¦‚æœé—®é¢˜åªæ¶‰åŠæŸä¸ªç‰¹å®šä¸»é¢˜ï¼ˆå¦‚å¤ªé˜³ã€æœ›è¿œé•œã€å¼€æ™®å‹’å®šå¾‹ç­‰ï¼‰ï¼Œåªé€‰æ‹©åŒ…å«è¯¥ä¸»é¢˜çš„æ–‡ä»¶

**é€‰æ‹©ç­–ç•¥**ï¼š
1. ä»”ç»†åˆ†æç”¨æˆ·é—®é¢˜ä¸­çš„**å…³é”®è¯**ï¼ˆå¦‚ï¼šå¤ªé˜³ã€æœ›è¿œé•œã€è½¨é“ã€å…‰è°±ç­‰ï¼‰
2. æŸ¥çœ‹æ¯ä¸ªæ–‡ä»¶çš„**å†…å®¹æ‘˜è¦**ï¼Œåˆ¤æ–­æ˜¯å¦åŒ…å«ç›¸å…³ä¸»é¢˜
3. **åªé€‰æ‹©**å†…å®¹æ‘˜è¦ä¸­æ˜ç¡®æåˆ°ç›¸å…³ä¸»é¢˜çš„æ–‡ä»¶
4. å¦‚æœå¤šä¸ªæ–‡ä»¶éƒ½ç›¸å…³ï¼Œå¯ä»¥é€‰æ‹©å¤šä¸ªï¼Œä½†è¦ç¡®ä¿æ¯ä¸ªéƒ½æ˜¯å¿…è¦çš„

**å¯ç”¨çš„æ–‡ä»¶åŠå…¶è¯¦ç»†å†…å®¹ï¼ˆå…± {len(files)} ä¸ªï¼‰**ï¼š

{chr(10).join(file_descriptions)}

**ç¤ºä¾‹**ï¼š
- é—®é¢˜ï¼š"å¤ªé˜³çš„å†…éƒ¨ç»“æ„" â†’ åªé€‰æ‹© ["301F09.Ch16.Sun.Slides.pdf"]ï¼ˆå› ä¸ºåªæœ‰å®ƒä»‹ç»å¤ªé˜³ï¼‰
- é—®é¢˜ï¼š"æœ›è¿œé•œçš„å·¥ä½œåŸç†" â†’ åªé€‰æ‹© ["301F09.TelescopesCh5.9.16.09.pdf"]ï¼ˆå› ä¸ºåªæœ‰å®ƒä»‹ç»æœ›è¿œé•œï¼‰
- é—®é¢˜ï¼š"å¼€æ™®å‹’å®šå¾‹" â†’ é€‰æ‹© ["301F09.IntroOrbitsLight.I.pdf", "301F09Scalo.IntOrbLight.II.pdf"]ï¼ˆä¸¤ä¸ªéƒ½æ¶‰åŠè½¨é“å’Œå¼€æ™®å‹’å®šå¾‹ï¼‰
- é—®é¢˜ï¼š"å…‰è°±åˆ†æ" â†’ åªé€‰æ‹© ["301F09.LecturesCh3.5_4.pdf"]ï¼ˆå› ä¸ºå®ƒä¸“é—¨è®²å…‰è°±ï¼‰

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å†…å®¹ï¼Œ**ç²¾ç¡®é€‰æ‹©**æœ€ç›¸å…³çš„æ–‡ä»¶ï¼Œé¿å…é€‰æ‹©ä¸å¿…è¦çš„æ–‡ä»¶ã€‚
"""
    
    return {
        "name": "select_relevant_files",
        "description": description,
        "parameters": {
            "type": "object",
            "properties": {
                "file_titles": {
                    "type": "array",
                    "items": {
                        "type": "string",
                        "enum": file_titles
                    },
                    "description": "éœ€è¦æŸ¥è¯¢çš„æ–‡ä»¶æ ‡é¢˜åˆ—è¡¨ï¼Œä»å¯ç”¨æ–‡ä»¶ä¸­é€‰æ‹©æœ€ç›¸å…³çš„ä¸€ä¸ªæˆ–å¤šä¸ª"
                },
            },
            "required": ["file_titles"],
        },
    }


# å®šä¹‰å‡½æ•°å£°æ˜
get_knowledge_level_function = {
    "name": "get_knowledge_level",
    "description": """è·å–ç”¨æˆ·åœ¨æŒ‡å®šå­¦ç§‘é¢†åŸŸçš„çŸ¥è¯†æ°´å¹³ä¿¡æ¯ï¼ˆåŒ…æ‹¬levelå’Œdetailed_descriptionï¼‰ã€‚
    
å½“ç”¨æˆ·æåˆ°ä»¥ä¸‹ä»»ä½•æƒ…å†µæ—¶ï¼Œå¿…é¡»è°ƒç”¨æ­¤å‡½æ•°ï¼š
- è¯¢é—®å­¦ä¹ äº†ä»€ä¹ˆå†…å®¹ã€çŸ¥è¯†ç‚¹ã€è¯¾ç¨‹å†…å®¹
- è¯¢é—®çŸ¥è¯†æ°´å¹³ã€æŒæ¡ç¨‹åº¦ã€å­¦ä¹ æƒ…å†µ
- æåˆ°å…·ä½“å­¦ç§‘åç§°ï¼ˆå¾®ç§¯åˆ†ã€ä»£æ•°ã€å¤©æ–‡å­¦ç­‰ï¼‰
- éœ€è¦æ ¹æ®ç”¨æˆ·æ°´å¹³æä¾›å»ºè®®æˆ–æ€»ç»“

å¯ç”¨çš„å­¦ç§‘ï¼š
- calculusï¼ˆå¾®ç§¯åˆ†ï¼‰ï¼šåŒ…æ‹¬å¯¼æ•°ã€ç§¯åˆ†ã€æé™ç­‰
- algebraï¼ˆä»£æ•°ï¼‰ï¼šåŒ…æ‹¬æ–¹ç¨‹ã€å‡½æ•°ã€è¡¨è¾¾å¼ç­‰
- astronomyï¼ˆå¤©æ–‡å­¦ï¼‰ï¼šåŒ…æ‹¬å¤©ä½“ã€æ˜Ÿç³»ã€è½¨é“åŠ›å­¦ç­‰
- general_scienceï¼ˆé€šç”¨ç§‘å­¦ï¼‰ï¼šç‰©ç†ã€åŒ–å­¦ã€ç”Ÿç‰©ç­‰åŸºç¡€ç§‘å­¦

ç¤ºä¾‹è§¦å‘åœºæ™¯ï¼š
- "è¯·å‘Šè¯‰æˆ‘è¿™å­¦æœŸå­¦äº†å“ªäº›å¤©æ–‡å­¦çŸ¥è¯†" â†’ è°ƒç”¨ get_knowledge_level(["astronomy"])
- "æˆ‘åœ¨å¾®ç§¯åˆ†æ–¹é¢çš„æ°´å¹³å¦‚ä½•" â†’ è°ƒç”¨ get_knowledge_level(["calculus"])
- "æ€»ç»“ä¸€ä¸‹æˆ‘çš„å­¦ä¹ æƒ…å†µ" â†’ è°ƒç”¨ get_knowledge_level(["calculus", "algebra", "astronomy", "general_science"])
""",
    "parameters": {
        "type": "object",
        "properties": {
            "subjects": {
                "type": "array",
                "items": {
                    "type": "string",
                    "enum": ["calculus", "algebra", "astronomy", "general_science"]
                },
                "description": "éœ€è¦æŸ¥è¯¢çš„å­¦ç§‘åˆ—è¡¨ã€‚å¯é€‰å€¼ï¼š'calculus'ï¼ˆå¾®ç§¯åˆ†ï¼‰ã€'algebra'ï¼ˆä»£æ•°ï¼‰ã€'astronomy'ï¼ˆå¤©æ–‡å­¦ï¼‰ã€'general_science'ï¼ˆé€šç”¨ç§‘å­¦ï¼‰"
            },
        },
        "required": ["subjects"],
    },
}

select_relevant_files_function = create_select_files_function()

generate_detailed_response_function = {
    "name": "generate_detailed_response",
    "description": """ä½¿ç”¨æµå¼å¤šè½®å¯¹è¯æ¨¡å‹ï¼ŒåŸºäºç”¨æˆ·çš„çŸ¥è¯†æ°´å¹³å’Œç›¸å…³æ–‡æ¡£ï¼Œç”Ÿæˆè¯¦ç»†ã€ä¸ªæ€§åŒ–çš„å›ç­”ã€‚

å½“ç”¨æˆ·æåˆ°ä»¥ä¸‹ä»»ä½•æƒ…å†µæ—¶ï¼Œåº”è¯¥è°ƒç”¨æ­¤å‡½æ•°ï¼š
- éœ€è¦åŸºäºæ–‡æ¡£å†…å®¹ç”Ÿæˆè¯¦ç»†è§£é‡Š
- éœ€è¦æ ¹æ®ç”¨æˆ·çŸ¥è¯†æ°´å¹³å®šåˆ¶å›ç­”
- å·²ç»è·å–äº†çŸ¥è¯†æ°´å¹³å’Œç›¸å…³æ–‡ä»¶ï¼Œéœ€è¦æ•´åˆä¿¡æ¯å›ç­”
- ç”¨æˆ·éœ€è¦æ·±å…¥ã€å…¨é¢çš„è§£ç­”

æ­¤å‡½æ•°ä¼šï¼š
- ç»“åˆç”¨æˆ·çš„çŸ¥è¯†æ°´å¹³è°ƒæ•´è§£é‡Šæ·±åº¦
- åŸºäºé€‰ä¸­çš„æ–‡æ¡£å†…å®¹æä¾›å‡†ç¡®ä¿¡æ¯
- ä»¥æµå¼æ–¹å¼è¾“å‡ºï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ
- æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯ä»¥è¿½é—®å’Œæ·±å…¥æ¢è®¨

è°ƒç”¨æ—¶æœºï¼š
- åœ¨è·å–äº† get_knowledge_level å’Œ/æˆ– select_relevant_files çš„ç»“æœå
- ä½œä¸ºæœ€åä¸€æ­¥ï¼Œæ•´åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆå›ç­”
""",
    "parameters": {
        "type": "object",
        "properties": {
            "user_query": {
                "type": "string",
                "description": "ç”¨æˆ·çš„åŸå§‹é—®é¢˜"
            },
            "use_knowledge_level": {
                "type": "boolean",
                "description": "æ˜¯å¦ä½¿ç”¨çŸ¥è¯†æ°´å¹³ä¿¡æ¯ï¼ˆå¦‚æœä¹‹å‰è°ƒç”¨äº† get_knowledge_levelï¼‰"
            },
            "use_selected_files": {
                "type": "boolean",
                "description": "æ˜¯å¦ä½¿ç”¨é€‰ä¸­çš„æ–‡ä»¶ä¿¡æ¯ï¼ˆå¦‚æœä¹‹å‰è°ƒç”¨äº† select_relevant_filesï¼‰"
            }
        },
        "required": ["user_query"],
    },
}


class GradioDirectorAgent:
    """
    Gradio ç•Œé¢çš„ Director Agent
    è´Ÿè´£åˆ†æç”¨æˆ·è¯·æ±‚ï¼Œè®°å½•å‡½æ•°è°ƒç”¨è·¯å¾„ï¼Œå¹¶ç”Ÿæˆæœ€ç»ˆå›å¤
    """
    
    def __init__(self):
        """åˆå§‹åŒ– Director Agent"""
        # ä»ç¯å¢ƒå˜é‡è·å– API Key
        api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("è¯·è®¾ç½® GOOGLE_API_KEY æˆ– GEMINI_API_KEY ç¯å¢ƒå˜é‡")
        
        self.client = genai.Client(api_key=api_key)
        self.memory_tool = MemoryTool()
        self.select_file_tool = SelectFileTool()
        self.response_generator = ResponseGeneratorTool()
        
        # å†³ç­–æ¨¡å‹çš„ç³»ç»ŸæŒ‡ä»¤ï¼ˆå·²åŒæ­¥ main.py çš„å¤šè½®å¯¹è¯ä¼˜åŒ–ï¼‰
        self.system_instruction = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å†³ç­–åŠ©æ‰‹ (Director Agent)ã€‚ä½ çš„èŒè´£æ˜¯åˆ†æç”¨æˆ·çš„è¯·æ±‚ï¼Œå¹¶å†³å®šéœ€è¦è°ƒç”¨å“ªäº›å·¥å…·æ¥æ»¡è¶³ç”¨æˆ·éœ€æ±‚ã€‚

**ğŸ”µ é‡è¦æç¤ºï¼šå¤šè½®å¯¹è¯èƒ½åŠ›**
- ä½ æ­£åœ¨å‚ä¸ä¸€ä¸ªå¤šè½®å¯¹è¯ä¼šè¯ï¼Œ**ä½ å¯ä»¥è®¿é—®å®Œæ•´çš„å¯¹è¯å†å²**
- å½“ç”¨æˆ·æåŠ"ä¸Šæ¬¡"ã€"åˆšæ‰"ã€"ä¹‹å‰"ç­‰è¯æ—¶ï¼Œä½ åº”è¯¥æŸ¥çœ‹å¯¹è¯å†å²æ¥å›ç­”
- å¯¹äºç®€å•çš„ä¸Šä¸‹æ–‡å¼•ç”¨é—®é¢˜ï¼ˆå¦‚"æˆ‘åˆšæ‰é—®äº†ä»€ä¹ˆ"ï¼‰ï¼Œä½ åº”è¯¥**ç›´æ¥å›ç­”**ï¼Œä¸éœ€è¦è°ƒç”¨ä»»ä½•å·¥å…·
- åªæœ‰å½“ç”¨æˆ·éœ€è¦æŸ¥è¯¢çŸ¥è¯†æ°´å¹³ã€æ–‡æ¡£å†…å®¹æˆ–ç”Ÿæˆè¯¦ç»†è§£é‡Šæ—¶ï¼Œæ‰è°ƒç”¨ç›¸åº”çš„å·¥å…·

ä½ å¯ä»¥è°ƒç”¨ä»¥ä¸‹ä¸‰ä¸ªå·¥å…·ï¼š

1. **get_knowledge_level** - è·å–ç”¨æˆ·çŸ¥è¯†æ°´å¹³
   - å½“ç”¨æˆ·çš„è¯·æ±‚ä¸­å¯ä»¥è¯†åˆ«å‡ºæ‰€å±çš„å­¦ç§‘ï¼Œå°±è¦å…ˆè°ƒç”¨æ­¤å‡½æ•°ï¼Œæ¥äº†è§£ç”¨æˆ·åœ¨è¯¥å­¦ç§‘çš„çŸ¥è¯†æ°´å¹³
   - å½“éœ€è¦æ ¹æ®ç”¨æˆ·æ°´å¹³å®šåˆ¶å›ç­”æ—¶è°ƒç”¨
   - å¯æŸ¥è¯¢å­¦ç§‘: calculus(å¾®ç§¯åˆ†), algebra(ä»£æ•°), astronomy(å¤©æ–‡å­¦), general_science(é€šç”¨ç§‘å­¦)

2. **select_relevant_files** - é€‰æ‹©ç›¸å…³æ–‡æ¡£
   - å½“ç”¨æˆ·éœ€è¦è¯¦ç»†äº†è§£å¤©æ–‡å­¦æ—¶
   - å½“éœ€è¦æŸ¥é˜…è¯¾ç¨‹èµ„æ–™æˆ–æ–‡æ¡£æ—¶è°ƒç”¨
   - ä»æ–‡æ¡£åº“ä¸­é€‰æ‹©æœ€ç›¸å…³çš„æ–‡ä»¶
   - âš ï¸ **é‡è¦**: æ–‡æ¡£åº“ä¸­åŒ…å«ç”¨æˆ·çš„è¯¾ç¨‹è¯¾ä»¶ï¼Œè¿™æ˜¯æœ€æƒå¨çš„å­¦ä¹ èµ„æ–™

3. **generate_detailed_response** - ç”Ÿæˆè¯¦ç»†å›ç­”
   - åœ¨è·å–äº†çŸ¥è¯†æ°´å¹³å’Œ/æˆ–ç›¸å…³æ–‡ä»¶åè°ƒç”¨
   - æ•´åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆä¸ªæ€§åŒ–çš„è¯¦ç»†å›ç­”
   - **å¿…é¡»**ä½œä¸ºæœ€åä¸€æ­¥è°ƒç”¨

**ğŸ”´ æ ¸å¿ƒå†³ç­–æµç¨‹ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰**ï¼š

**è§„åˆ™0ï¼šä¼˜å…ˆå¤„ç†ä¸Šä¸‹æ–‡æŸ¥è¯¢**
- âœ… å¦‚æœç”¨æˆ·åªæ˜¯è¯¢é—®å¯¹è¯å†å²ï¼ˆå¦‚"æˆ‘åˆšæ‰é—®äº†ä»€ä¹ˆ"ã€"ä¸Šä¸ªé—®é¢˜æ˜¯ä»€ä¹ˆ"ï¼‰ï¼Œ**ç›´æ¥æŸ¥çœ‹å¯¹è¯å†å²å¹¶å›ç­”**
- âœ… ä¸è¦ä¸ºç®€å•çš„ä¸Šä¸‹æ–‡æŸ¥è¯¢è°ƒç”¨ä»»ä½•å·¥å…·
- ç¤ºä¾‹ï¼š
  - "æˆ‘ä¸Šæ¬¡é—®äº†ä»€ä¹ˆï¼Ÿ" â†’ æŸ¥çœ‹å¯¹è¯å†å²ï¼Œç›´æ¥å›ç­”ï¼ˆä¸è°ƒç”¨å·¥å…·ï¼‰
  - "åˆšæ‰ä½ è¯´äº†ä»€ä¹ˆï¼Ÿ" â†’ æŸ¥çœ‹å¯¹è¯å†å²ï¼Œç›´æ¥å›ç­”ï¼ˆä¸è°ƒç”¨å·¥å…·ï¼‰

**è§„åˆ™1ï¼šè¯†åˆ«å­¦ç§‘ç±»å‹**
- åˆ†æç”¨æˆ·é—®é¢˜ï¼Œè¯†åˆ«æ¶‰åŠçš„å­¦ç§‘ï¼ˆcalculusã€algebraã€astronomyã€general_scienceï¼‰
- åªè¦é—®é¢˜æ¶‰åŠä»»ä½•å­¦ç§‘çŸ¥è¯†ï¼Œéƒ½å¿…é¡»æ‰§è¡Œè§„åˆ™2

**è§„åˆ™2ï¼šå¿…é¡»å…ˆè·å–çŸ¥è¯†æ°´å¹³**
- âœ… åªè¦ç”¨æˆ·è¯·æ±‚åŒ…å«å­¦ç§‘ç›¸å…³çš„çŸ¥è¯†ï¼Œ**ç¬¬ä¸€æ­¥å¿…é¡»è°ƒç”¨ get_knowledge_level**
- âœ… å…ˆäº†è§£ç”¨æˆ·çš„çŸ¥è¯†ç­‰çº§ï¼Œæ‰èƒ½æä¾›ä¸ªæ€§åŒ–å›ç­”

**è§„åˆ™3ï¼šå¤©æ–‡å­¦å¿…é¡»æŸ¥é˜…æ–‡æ¡£**
- âœ… å¦‚æœé—®é¢˜æ¶‰åŠå¤©æ–‡å­¦ï¼ˆastronomyï¼‰ï¼Œ**å¿…é¡»**åœ¨è·å–çŸ¥è¯†æ°´å¹³åè°ƒç”¨ select_relevant_files
- âœ… é€‰æ‹©æ‰€æœ‰ä¸é—®é¢˜ç›¸å…³çš„å¤©æ–‡å­¦è¯¾ä»¶æ–‡ä»¶

**è§„åˆ™4ï¼šæœ€åç”Ÿæˆå›å¤**
- âœ… **å¿…é¡»**æœ€åè°ƒç”¨ generate_detailed_response
- âœ… æ•´åˆçŸ¥è¯†æ°´å¹³å’Œæ–‡æ¡£ä¿¡æ¯ç”Ÿæˆä¸ªæ€§åŒ–å›ç­”

**ğŸ¯ å…·ä½“ç¤ºä¾‹ï¼ˆå¿…é¡»éµå¾ªï¼‰**ï¼š

ç¤ºä¾‹ 1: "æˆ‘åˆšæ‰é—®äº†ä»€ä¹ˆï¼Ÿ"
   - è¯†åˆ«ï¼šè¿™æ˜¯ä¸Šä¸‹æ–‡æŸ¥è¯¢
   - æ‰§è¡Œæµç¨‹ï¼šç›´æ¥æŸ¥çœ‹å¯¹è¯å†å²å¹¶å›ç­”ï¼ˆä¸è°ƒç”¨ä»»ä½•å·¥å…·ï¼‰

ç¤ºä¾‹ 2: "è§£é‡Šå¤ªé˜³çš„å†…éƒ¨ç»“æ„"
   - è¯†åˆ«ï¼šæ¶‰åŠå¤©æ–‡å­¦
   - æ‰§è¡Œæµç¨‹ï¼š
     â‘  get_knowledge_level(["astronomy"])
     â‘¡ select_relevant_files(["301F09.Ch16.Sun.Slides.pdf"])
     â‘¢ generate_detailed_response(use_knowledge=True, use_files=True)

ç¤ºä¾‹ 3: "æˆ‘ä¸Šæ¬¡é—®çš„é‚£ä¸ªé—®é¢˜èƒ½è¯¦ç»†è§£é‡Šä¸€ä¸‹å—ï¼Ÿ"
   - è¯†åˆ«ï¼šå¼•ç”¨äº†ä¸Šä¸‹æ–‡ + éœ€è¦è¯¦ç»†è§£é‡Š
   - æ‰§è¡Œæµç¨‹ï¼š
     â‘  æŸ¥çœ‹å¯¹è¯å†å²ï¼Œç¡®å®š"ä¸Šæ¬¡é—®çš„é—®é¢˜"æ˜¯ä»€ä¹ˆ
     â‘¡ æ ¹æ®é‚£ä¸ªé—®é¢˜ï¼Œè°ƒç”¨ç›¸åº”çš„å·¥å…·
     â‘¢ è°ƒç”¨ generate_detailed_response ç”Ÿæˆè¯¦ç»†è§£é‡Š

**âš ï¸ é‡è¦æé†’**ï¼š
- æ¯æ¬¡è°ƒç”¨å‡½æ•°æ—¶ï¼Œåªè°ƒç”¨ä¸€ä¸ªå‡½æ•°ï¼Œç­‰å¾…ç»“æœåå†å†³å®šä¸‹ä¸€æ­¥
- ä¸è¦ä¸€æ¬¡æ€§è°ƒç”¨å¤šä¸ªå‡½æ•°
- ä¸¥æ ¼æŒ‰ç…§æµç¨‹é¡ºåºæ‰§è¡Œ
- å¤©æ–‡å­¦é—®é¢˜å¿…é¡»æ‰§è¡Œå®Œæ•´çš„æµç¨‹ï¼ˆä¸‰ä¸ªæ­¥éª¤ï¼‰
"""
        
        # é…ç½®å·¥å…·
        self.tools = types.Tool(function_declarations=[
            get_knowledge_level_function,
            select_relevant_files_function,
            generate_detailed_response_function
        ])
        
        self.config = types.GenerateContentConfig(
            tools=[self.tools],
            system_instruction=self.system_instruction
        )
    
    def execute_function(self, function_call, user_query: str, 
                        knowledge_level_result: dict = None, 
                        selected_files_result: dict = None):
        """æ‰§è¡Œå‡½æ•°è°ƒç”¨å¹¶è¿”å›ç»“æœ"""
        if function_call.name == "get_knowledge_level":
            subjects = list(function_call.args.get("subjects", []))
            result = self.memory_tool.get_knowledge_level(subjects)
            return result
            
        elif function_call.name == "select_relevant_files":
            file_titles = list(function_call.args.get("file_titles", []))
            result = self.select_file_tool.select_files_by_titles(file_titles)
            return result
            
        elif function_call.name == "generate_detailed_response":
            query = function_call.args.get("user_query", user_query)
            use_knowledge = function_call.args.get("use_knowledge_level", False)
            use_files = function_call.args.get("use_selected_files", False)
            
            knowledge_info = knowledge_level_result if use_knowledge else None
            files_info = selected_files_result.get("selected_files", []) if use_files and selected_files_result else None
            
            # ä¸å†éœ€è¦ file_urisï¼Œç›´æ¥ä½¿ç”¨ files_info ä¸­çš„ file_path
            detailed_response = self.response_generator.generate_response_stream(
                user_query=query,
                knowledge_level_info=knowledge_info,
                selected_files=files_info
            )
            
            result = {
                "response": detailed_response,
                "used_knowledge_level": use_knowledge,
                "used_files": use_files,
                "response_length": len(detailed_response)
            }
            
            return result
        
        return None
    
    def process_query_with_chat(self, user_query: str, chat=None):
        """
        ä½¿ç”¨èŠå¤©ä¼šè¯å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰
        
        Args:
            user_query: ç”¨æˆ·çš„é—®é¢˜
            chat: èŠå¤©ä¼šè¯å¯¹è±¡ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            str: AI çš„å›å¤æ–‡æœ¬
        """
        if not user_query or not user_query.strip():
            return "âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜"
        
        # ç”¨äºå­˜å‚¨å‡½æ•°è°ƒç”¨ç»“æœ
        knowledge_level_result = None
        selected_files_result = None
        
        try:
            # å¦‚æœä½¿ç”¨èŠå¤©ä¼šè¯ï¼Œç›´æ¥å‘é€æ¶ˆæ¯
            if chat:
                response = chat.send_message(user_query)
            else:
                # å¦åˆ™ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
                conversation_history = [
                    types.Content(
                        role="user",
                        parts=[types.Part(text=user_query)]
                    )
                ]
                
                response = self.client.models.generate_content(
                    model="gemini-2.0-flash",
                    contents=conversation_history,
                    config=self.config,
                )
            
            # å¾ªç¯å¤„ç†å‡½æ•°è°ƒç”¨
            max_iterations = 10
            iteration = 0
            
            while iteration < max_iterations:
                iteration += 1
                
                # æå–å‡½æ•°è°ƒç”¨
                function_calls_list = []
                if response.candidates and len(response.candidates) > 0:
                    parts = response.candidates[0].content.parts
                    if parts and len(parts) > 0:
                        for part in parts:
                            if hasattr(part, 'function_call') and part.function_call:
                                function_calls_list.append(part.function_call)
                
                if not function_calls_list:
                    # æ²¡æœ‰å‡½æ•°è°ƒç”¨ï¼Œè¿”å›æœ€ç»ˆå›å¤
                    return response.text if response.text else "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"
                
                # å¤„ç†æ‰€æœ‰å‡½æ•°è°ƒç”¨
                function_responses = []
                
                for function_call in function_calls_list:
                    # æ‰§è¡Œå‡½æ•°è°ƒç”¨
                    result = self.execute_function(
                        function_call, 
                        user_query, 
                        knowledge_level_result, 
                        selected_files_result
                    )
                    
                    # ä¿å­˜ç»“æœ
                    if function_call.name == "get_knowledge_level":
                        knowledge_level_result = result
                    elif function_call.name == "select_relevant_files":
                        selected_files_result = result
                    elif function_call.name == "generate_detailed_response":
                        # å¦‚æœè°ƒç”¨äº† generate_detailed_responseï¼Œä»»åŠ¡å®Œæˆ
                        return result["response"]
                    
                    # åˆ›å»ºå‡½æ•°å“åº”
                    function_response = types.Part.from_function_response(
                        name=function_call.name,
                        response=result
                    )
                    function_responses.append(function_response)
                
                # å‘é€å‡½æ•°å“åº”å¹¶è·å–æ–°çš„å›å¤
                if chat:
                    response = chat.send_message(function_responses)
                else:
                    conversation_history.append(response.candidates[0].content)
                    conversation_history.append(
                        types.Content(
                            role="user",
                            parts=function_responses
                        )
                    )
                    response = self.client.models.generate_content(
                        model="gemini-2.0-flash",
                        contents=conversation_history,
                        config=self.config,
                    )
            
            return "âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°é™åˆ¶"
            
        except Exception as e:
            return f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
    
    def process_query_for_gradio(self, user_query: str):
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢å¹¶è¿”å›æ ¼å¼åŒ–çš„ç»“æœ
        
        Returns:
            tuple: (å‡½æ•°è°ƒç”¨è·¯å¾„, æœ€ç»ˆå›å¤)
        """
        if not user_query or not user_query.strip():
            return "âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜", ""
        
        # ç”¨äºè®°å½•å‡½æ•°è°ƒç”¨è·¯å¾„
        function_call_log = []
        function_call_log.append(f"### ğŸ“ ç”¨æˆ·é—®é¢˜\n\n{user_query}\n")
        function_call_log.append("\n### ğŸ§  Director Agent åˆ†æè·¯å¾„\n")
        
        # ç”¨äºå­˜å‚¨å‡½æ•°è°ƒç”¨ç»“æœ
        knowledge_level_result = None
        selected_files_result = None
        final_response = ""
        
        # æ„å»ºå¯¹è¯å†å²
        conversation_history = [
            types.Content(
                role="user",
                parts=[types.Part(text=user_query)]
            )
        ]
        
        # å¾ªç¯å¤„ç†
        max_iterations = 10
        iteration = 0
        step_number = 1
        
        try:
            while iteration < max_iterations:
                iteration += 1
                
                # è°ƒç”¨æ¨¡å‹
                response = self.client.models.generate_content(
                    model="gemini-2.0-flash",
                    contents=conversation_history,
                    config=self.config,
                )
                
                # æå–å‡½æ•°è°ƒç”¨
                function_calls_list = []
                if response.candidates and len(response.candidates) > 0:
                    parts = response.candidates[0].content.parts
                    if parts and len(parts) > 0:
                        for part in parts:
                            if hasattr(part, 'function_call') and part.function_call:
                                function_calls_list.append(part.function_call)
                
                if not function_calls_list:
                    # æ²¡æœ‰å‡½æ•°è°ƒç”¨ï¼Œå¾—åˆ°æœ€ç»ˆå›å¤
                    if response.text:
                        final_response = response.text
                    break
                
                # å°†æ¨¡å‹çš„å“åº”æ·»åŠ åˆ°å¯¹è¯å†å²
                conversation_history.append(response.candidates[0].content)
                
                # å¤„ç†æ‰€æœ‰å‡½æ•°è°ƒç”¨
                function_responses = []
                
                for function_call in function_calls_list:
                    # è®°å½•å‡½æ•°è°ƒç”¨
                    function_call_log.append(f"\n#### æ­¥éª¤ {step_number}: è°ƒç”¨å‡½æ•° `{function_call.name}`\n")
                    
                    # æ ¼å¼åŒ–å‚æ•°
                    args_dict = dict(function_call.args)
                    function_call_log.append(f"**å‚æ•°:**\n```json\n{json.dumps(args_dict, ensure_ascii=False, indent=2)}\n```\n")
                    
                    # æ‰§è¡Œå‡½æ•°è°ƒç”¨
                    result = self.execute_function(
                        function_call, 
                        user_query, 
                        knowledge_level_result, 
                        selected_files_result
                    )
                    
                    # è®°å½•ç»“æœ
                    if function_call.name == "get_knowledge_level":
                        knowledge_level_result = result
                        function_call_log.append("**ç»“æœ:**\n")
                        for subject, info in result["subjects_info"].items():
                            function_call_log.append(f"- **{subject}**: {info['level']} - {info['detailed_description']}\n")
                    
                    elif function_call.name == "select_relevant_files":
                        selected_files_result = result
                        function_call_log.append("**ç»“æœ:**\n")
                        function_call_log.append(f"- é€‰ä¸­æ–‡ä»¶æ•°: {len(result['selected_files'])}\n")
                        for file_info in result["selected_files"]:
                            function_call_log.append(f"  - ğŸ“„ {file_info['title']}\n")
                    
                    elif function_call.name == "generate_detailed_response":
                        final_response = result["response"]
                        function_call_log.append("**ç»“æœ:**\n")
                        function_call_log.append(f"- âœ… ç”Ÿæˆäº†è¯¦ç»†å›å¤ï¼ˆ{result['response_length']} å­—ç¬¦ï¼‰\n")
                        function_call_log.append(f"- ä½¿ç”¨çŸ¥è¯†æ°´å¹³: {'æ˜¯' if result['used_knowledge_level'] else 'å¦'}\n")
                        function_call_log.append(f"- ä½¿ç”¨æ–‡æ¡£: {'æ˜¯' if result['used_files'] else 'å¦'}\n")
                    
                    step_number += 1
                    
                    # åˆ›å»ºå‡½æ•°å“åº”
                    function_response = types.Part.from_function_response(
                        name=function_call.name,
                        response=result
                    )
                    function_responses.append(function_response)
                
                # å¦‚æœè°ƒç”¨äº† generate_detailed_responseï¼Œä»»åŠ¡å®Œæˆ
                if any(fc.name == "generate_detailed_response" for fc in function_calls_list):
                    break
                
                # å°†å‡½æ•°å“åº”æ·»åŠ åˆ°å¯¹è¯å†å²
                conversation_history.append(
                    types.Content(
                        role="user",
                        parts=function_responses
                    )
                )
            
            # è¿”å›ç»“æœ
            path_text = "\n".join(function_call_log)
            return path_text, final_response
            
        except Exception as e:
            error_msg = f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
            return error_msg, ""


def create_gradio_interface():
    """åˆ›å»º Gradio èŠå¤©ç•Œé¢ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰"""
    # å°è¯•åŠ è½½ç¯å¢ƒå˜é‡
    try:
        from dotenv import load_dotenv
        load_dotenv()
    except ImportError:
        pass
    
    # åˆå§‹åŒ– Director Agent
    try:
        agent = GradioDirectorAgent()
    except ValueError as e:
        print(f"é”™è¯¯: {e}")
        print("è¯·è®¾ç½® GOOGLE_API_KEY æˆ– GEMINI_API_KEY ç¯å¢ƒå˜é‡")
        sys.exit(1)
    
    # åˆ›å»º Gradio èŠå¤©ç•Œé¢
    with gr.Blocks(title="Director Agent èŠå¤©ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ’¬ Director Agent æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ
        
        æ”¯æŒ**çœŸæ­£çš„å¤šè½®å¯¹è¯**ï¼ç³»ç»Ÿä¼šè®°ä½å¯¹è¯å†å²ï¼Œå¯ä»¥ç†è§£ä¸Šä¸‹æ–‡ã€‚
        
        **ç³»ç»Ÿèƒ½åŠ›:**
        - ğŸ§  æ™ºèƒ½åˆ†æç”¨æˆ·è¯·æ±‚ï¼Œè°ƒç”¨åˆé€‚çš„å·¥å…·
        - ğŸ“š è·å–ç”¨æˆ·çŸ¥è¯†æ°´å¹³ï¼ˆå¾®ç§¯åˆ†ã€ä»£æ•°ã€å¤©æ–‡å­¦ã€é€šç”¨ç§‘å­¦ï¼‰
        - ğŸ“„ é€‰æ‹©ç›¸å…³æ–‡æ¡£ï¼ˆå¤©æ–‡å­¦è¯¾ä»¶ï¼‰
        - ğŸ¤– ç”Ÿæˆä¸ªæ€§åŒ–çš„è¯¦ç»†å›å¤
        - ğŸ’¬ æ”¯æŒå¤šè½®å¯¹è¯ï¼Œç†è§£ä¸Šä¸‹æ–‡å¼•ç”¨
        """)
        
        # ä½¿ç”¨ gr.State ä¿å­˜èŠå¤©ä¼šè¯
        chat_session = gr.State(None)
        
        # èŠå¤©ç•Œé¢
        chatbot = gr.Chatbot(
            label="å¯¹è¯å†å²",
            height=500,
            show_label=True,
            bubble_full_width=False
        )
        
        with gr.Row():
            msg = gr.Textbox(
                label="è¾“å…¥æ¶ˆæ¯",
                placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼šæˆ‘åœ¨å¤©æ–‡å­¦æ–¹é¢å­¦äº†ä»€ä¹ˆï¼Ÿ",
                scale=4,
                show_label=False
            )
            submit_btn = gr.Button("å‘é€ ğŸ“¤", variant="primary", scale=1)
        
        with gr.Row():
            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤å†å²", variant="secondary")
            retry_btn = gr.Button("ğŸ”„ é‡è¯•ä¸Šä¸€æ¡", variant="secondary")
        
        gr.Markdown("""
        ### ğŸ’¡ ç¤ºä¾‹å¯¹è¯:
        
        **è¿ç»­æé—®ç¤ºä¾‹ï¼š**
        1. "æˆ‘åœ¨å¤©æ–‡å­¦æ–¹é¢å­¦äº†ä»€ä¹ˆï¼Ÿ"
        2. "è¯¦ç»†è§£é‡Šä¸€ä¸‹å¤ªé˜³çš„å†…éƒ¨ç»“æ„"
        3. "æˆ‘åˆšæ‰é—®äº†ä»€ä¹ˆï¼Ÿ" â† AI ä¼šè®°ä½å¹¶å›ç­”
        4. "èƒ½å†ç®€å•ç‚¹è§£é‡Šå—ï¼Ÿ" â† AI ç†è§£æ˜¯æŒ‡å¤ªé˜³çš„å†…éƒ¨ç»“æ„
        
        **å…¶ä»–ç¤ºä¾‹é—®é¢˜ï¼š**
        - ç»™æˆ‘æ€»ç»“ä¸€ä¸‹å¼€æ™®å‹’å®šå¾‹
        - æœ›è¿œé•œçš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ
        - æˆ‘è¿™å­¦æœŸå­¦äº†å“ªäº›å¤©æ–‡å­¦çŸ¥è¯†ï¼Ÿ
        """)
        
        # åˆå§‹åŒ–æˆ–è·å–èŠå¤©ä¼šè¯
        def get_or_create_chat(chat):
            if chat is None:
                chat = agent.client.chats.create(
                    model="gemini-2.0-flash",
                    config=agent.config
                )
            return chat
        
        # å¤„ç†ç”¨æˆ·æ¶ˆæ¯
        def respond(message, history, chat):
            if not message or not message.strip():
                return history, "", chat
            
            # è·å–æˆ–åˆ›å»ºèŠå¤©ä¼šè¯
            chat = get_or_create_chat(chat)
            
            # è°ƒç”¨ DirectorAgent å¤„ç†
            response = agent.process_query_with_chat(message, chat)
            
            # æ›´æ–°å†å²
            history = history or []
            history.append((message, response))
            
            return history, "", chat
        
        # æ¸…é™¤å†å²
        def clear_history():
            return [], None
        
        # é‡è¯•ä¸Šä¸€æ¡
        def retry_last(history, chat):
            if not history or len(history) == 0:
                return history, "", chat
            
            # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            last_message = history[-1][0]
            
            # åˆ é™¤æœ€åä¸€æ¡å¯¹è¯
            history = history[:-1]
            
            # é‡æ–°å‘é€
            return respond(last_message, history, chat)
        
        # ç»‘å®šäº‹ä»¶
        submit_btn.click(
            fn=respond,
            inputs=[msg, chatbot, chat_session],
            outputs=[chatbot, msg, chat_session]
        )
        
        msg.submit(
            fn=respond,
            inputs=[msg, chatbot, chat_session],
            outputs=[chatbot, msg, chat_session]
        )
        
        clear_btn.click(
            fn=clear_history,
            outputs=[chatbot, chat_session]
        )
        
        retry_btn.click(
            fn=retry_last,
            inputs=[chatbot, chat_session],
            outputs=[chatbot, msg, chat_session]
        )
    
    return demo


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    demo = create_gradio_interface()
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False,
        show_error=True
    )


if __name__ == "__main__":
    main()
