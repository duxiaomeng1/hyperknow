"""
å“åº”ç”Ÿæˆå·¥å…·ï¼šä½¿ç”¨æµå¼å¤šè½®å¯¹è¯æ¨¡å‹ç”Ÿæˆè¯¦ç»†å›ç­”
åŸºäºç”¨æˆ·è¯·æ±‚ã€çŸ¥è¯†æ°´å¹³å’Œç›¸å…³æ–‡æ¡£ç”Ÿæˆä¸ªæ€§åŒ–å›ç­”
"""
from google import genai
from google.genai import types
import os
import sys


class ResponseGeneratorTool:
    """å“åº”ç”Ÿæˆå·¥å…·ç±» - æ”¯æŒæµå¼è¾“å‡ºå’Œå¤šè½®å¯¹è¯"""
    
    def __init__(self, model_name: str = "gemini-2.0-flash"):
        """
        åˆå§‹åŒ–å“åº”ç”Ÿæˆå·¥å…·
        
        Args:
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.client = self._initialize_client()
        self.chat = None
        self.conversation_history = []
    
    def _initialize_client(self) -> genai.Client:
        """åˆå§‹åŒ– Gemini å®¢æˆ·ç«¯"""
        api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("è¯·è®¾ç½® GOOGLE_API_KEY æˆ– GEMINI_API_KEY ç¯å¢ƒå˜é‡")
        return genai.Client(api_key=api_key)
    
    def create_chat_session(self):
        """åˆ›å»ºæ–°çš„èŠå¤©ä¼šè¯"""
        self.chat = self.client.chats.create(model=self.model_name)
        self.conversation_history = []
        return self.chat
    
    def generate_response_stream(
        self,
        user_query: str,
        knowledge_level_info: dict = None,
        selected_files: list = None,
        file_uris: list = None
    ) -> str:
        """
        ç”Ÿæˆæµå¼å“åº”
        
        Args:
            user_query: ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢
            knowledge_level_info: ç”¨æˆ·çš„çŸ¥è¯†æ°´å¹³ä¿¡æ¯
            selected_files: é€‰ä¸­çš„æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
            file_uris: æ–‡ä»¶çš„ URI åˆ—è¡¨ï¼ˆå·²å¼ƒç”¨ï¼Œæ”¹ç”¨æœ¬åœ°æ–‡ä»¶ä¸Šä¼ ï¼‰
        
        Returns:
            å®Œæ•´çš„å“åº”æ–‡æœ¬
        """
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        context_parts = []
        
        # æ·»åŠ ç”¨æˆ·çŸ¥è¯†æ°´å¹³ä¿¡æ¯
        if knowledge_level_info:
            context = self._format_knowledge_level_context(knowledge_level_info)
            context_parts.append(context)
        
        # æ„å»ºå®Œæ•´çš„æç¤º
        prompt = self._build_prompt(user_query, context_parts)
        
        # å‘é€æ¶ˆæ¯å¹¶è·å–æµå¼å“åº”
        print("\n" + "="*60)
        print("ğŸ’¬ AI åŠ©æ‰‹çš„è¯¦ç»†å›ç­”:")
        print("="*60 + "\n")
        
        full_response = ""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ¬åœ°æ–‡ä»¶éœ€è¦ä¸Šä¼ 
            if selected_files and len(selected_files) > 0:
                # ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ä¸Šä¼ 
                content_parts = []
                
                # ä¸Šä¼ å¹¶æ·»åŠ æ‰€æœ‰æ–‡ä»¶
                for file_info in selected_files:
                    file_path = file_info.get("file_path")
                    if file_path and os.path.exists(file_path):
                        print(f"ğŸ“¤ æ­£åœ¨ä¸Šä¼ æ–‡ä»¶: {file_info.get('title', file_path)}")
                        try:
                            # ä¸Šä¼ æ–‡ä»¶å¹¶è·å– file å¯¹è±¡
                            # æ ¹æ®å®˜æ–¹æ–‡æ¡£: client.files.upload(file="path/to/file")
                            uploaded_file = self.client.files.upload(file=file_path)
                            print(f"âœ“ æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {uploaded_file.name}")
                            
                            # æ·»åŠ æ–‡ä»¶åˆ° content_parts
                            content_parts.append(types.Part.from_uri(
                                file_uri=uploaded_file.uri,
                                mime_type="application/pdf"
                            ))
                        except Exception as e:
                            print(f"âš ï¸ æ–‡ä»¶ä¸Šä¼ å¤±è´¥ ({file_info.get('title', file_path)}): {e}")
                            # å¦‚æœä¸Šä¼ å¤±è´¥ï¼Œä½¿ç”¨æ–‡ä»¶æ‘˜è¦ä½œä¸ºå¤‡é€‰
                            context = self._format_single_file_context(file_info)
                            prompt = context + "\n" + prompt
                    else:
                        # æ–‡ä»¶è·¯å¾„æ— æ•ˆï¼Œä½¿ç”¨æ‘˜è¦
                        print(f"âš ï¸ æ–‡ä»¶è·¯å¾„æ— æ•ˆï¼Œä½¿ç”¨æ‘˜è¦: {file_info.get('title', 'unknown')}")
                        context = self._format_single_file_context(file_info)
                        prompt = context + "\n" + prompt
                
                # æ·»åŠ æ–‡æœ¬æç¤º
                content_parts.append(types.Part(text=prompt))
                
                # ä½¿ç”¨æµå¼ç”Ÿæˆï¼ˆå¦‚æœæœ‰æˆåŠŸä¸Šä¼ çš„æ–‡ä»¶ï¼‰
                if len(content_parts) > 1:  # è‡³å°‘æœ‰ä¸€ä¸ªæ–‡ä»¶ + æ–‡æœ¬
                    response_stream = self.client.models.generate_content_stream(
                        model=self.model_name,
                        contents=content_parts
                    )
                    
                    for chunk in response_stream:
                        if chunk.text:
                            print(chunk.text, end="", flush=True)
                            full_response += chunk.text
                    print("\n")
                else:
                    # æ²¡æœ‰æˆåŠŸä¸Šä¼ çš„æ–‡ä»¶ï¼Œä½¿ç”¨èŠå¤©ä¼šè¯
                    if self.chat is None:
                        self.create_chat_session()
                    
                    response_stream = self.chat.send_message_stream(prompt)
                    for chunk in response_stream:
                        if chunk.text:
                            print(chunk.text, end="", flush=True)
                            full_response += chunk.text
                    print("\n")
            else:
                # æ²¡æœ‰æ–‡ä»¶ï¼Œä½¿ç”¨èŠå¤©ä¼šè¯
                if self.chat is None:
                    self.create_chat_session()
                
                response_stream = self.chat.send_message_stream(prompt)
                for chunk in response_stream:
                    if chunk.text:
                        print(chunk.text, end="", flush=True)
                        full_response += chunk.text
                print("\n")
                
        except Exception as e:
            print(f"\né”™è¯¯: ç”Ÿæˆå“åº”æ—¶å‡ºç°é—®é¢˜: {e}")
            full_response = f"æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºç°é”™è¯¯: {e}"
        
        # ä¿å­˜åˆ°å†å²è®°å½•
        self.conversation_history.append({
            "role": "user",
            "content": prompt
        })
        self.conversation_history.append({
            "role": "assistant",
            "content": full_response
        })
        
        return full_response
    
    def _format_knowledge_level_context(self, knowledge_level_info: dict) -> str:
        """æ ¼å¼åŒ–çŸ¥è¯†æ°´å¹³ä¿¡æ¯ä¸ºä¸Šä¸‹æ–‡"""
        context = "\nã€ç”¨æˆ·çŸ¥è¯†æ°´å¹³èƒŒæ™¯ã€‘\n"
        
        subjects_info = knowledge_level_info.get("subjects_info", {})
        for subject, info in subjects_info.items():
            level = info.get("level", "unknown")
            description = info.get("detailed_description", "")
            context += f"- {subject}: {level} æ°´å¹³\n"
            if description:
                context += f"  è¯¦æƒ…: {description}\n"
        
        context += "\nè¯·æ ¹æ®ç”¨æˆ·çš„çŸ¥è¯†æ°´å¹³ï¼Œç”¨åˆé€‚çš„æ–¹å¼è§£é‡Šæ¦‚å¿µã€‚\n"
        return context
    
    def _format_files_context(self, selected_files: list) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶ä¿¡æ¯ä¸ºä¸Šä¸‹æ–‡"""
        context = "\nã€ç›¸å…³å‚è€ƒæ–‡æ¡£ã€‘\n"
        
        for file_info in selected_files:
            title = file_info.get("title", "æœªçŸ¥æ–‡ä»¶")
            summary = file_info.get("content_summary", "")
            topics = file_info.get("topics", [])
            
            context += f"\næ–‡æ¡£: {title}\n"
            if topics:
                context += f"ä¸»é¢˜: {', '.join(topics)}\n"
            if summary:
                context += f"å†…å®¹æ‘˜è¦: {summary}\n"
        
        context += "\nè¯·åŸºäºä¸Šè¿°å‚è€ƒæ–‡æ¡£çš„å†…å®¹æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
        return context
    
    def _format_single_file_context(self, file_info: dict) -> str:
        """æ ¼å¼åŒ–å•ä¸ªæ–‡ä»¶ä¿¡æ¯ä¸ºä¸Šä¸‹æ–‡"""
        title = file_info.get("title", "æœªçŸ¥æ–‡ä»¶")
        summary = file_info.get("content_summary", "")
        topics = file_info.get("topics", [])
        
        context = f"\nã€å‚è€ƒæ–‡æ¡£: {title}ã€‘\n"
        if topics:
            context += f"ä¸»é¢˜: {', '.join(topics)}\n"
        if summary:
            context += f"å†…å®¹æ‘˜è¦: {summary}\n"
        context += "\n"
        return context
    
    def _build_prompt(self, user_query: str, context_parts: list) -> str:
        """æ„å»ºå®Œæ•´çš„æç¤º"""
        prompt = ""
        
        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        if context_parts:
            prompt += "".join(context_parts)
            prompt += "\n" + "-"*60 + "\n\n"
        
        # æ·»åŠ ç”¨æˆ·æŸ¥è¯¢
        prompt += f"ç”¨æˆ·é—®é¢˜: {user_query}\n\n"
        prompt += "è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®ã€æ˜“æ‡‚çš„å›ç­”ã€‚"
        
        return prompt
    
    def get_history(self) -> list:
        """è·å–å¯¹è¯å†å²"""
        if self.chat:
            try:
                return list(self.chat.get_history())
            except:
                return self.conversation_history
        return self.conversation_history
    
    def print_history(self):
        """æ‰“å°å¯¹è¯å†å²"""
        print("\n" + "="*60)
        print("ğŸ“œ å¯¹è¯å†å²:")
        print("="*60 + "\n")
        
        history = self.get_history()
        for i, message in enumerate(history, 1):
            role = message.role if hasattr(message, 'role') else message.get('role', 'unknown')
            print(f"[{i}] {role.upper()}:")
            
            if hasattr(message, 'parts'):
                for part in message.parts:
                    if hasattr(part, 'text'):
                        print(f"  {part.text[:200]}...")
            elif 'content' in message:
                print(f"  {message['content'][:200]}...")
            print()
    
    def continue_conversation(self, follow_up_query: str) -> str:
        """
        ç»§ç»­å¤šè½®å¯¹è¯
        
        Args:
            follow_up_query: åç»­é—®é¢˜
            
        Returns:
            å“åº”æ–‡æœ¬
        """
        if self.chat is None:
            raise ValueError("æ²¡æœ‰æ´»åŠ¨çš„èŠå¤©ä¼šè¯ï¼Œè¯·å…ˆè°ƒç”¨ generate_response_stream")
        
        print("\n" + "="*60)
        print("ğŸ’¬ AI åŠ©æ‰‹çš„å›ç­”:")
        print("="*60 + "\n")
        
        full_response = ""
        try:
            response_stream = self.chat.send_message_stream(follow_up_query)
            for chunk in response_stream:
                if chunk.text:
                    print(chunk.text, end="", flush=True)
                    full_response += chunk.text
            print("\n")
        except Exception as e:
            print(f"\né”™è¯¯: ç”Ÿæˆå“åº”æ—¶å‡ºç°é—®é¢˜: {e}")
            full_response = f"æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºç°é”™è¯¯: {e}"
        
        return full_response


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("æµ‹è¯• ResponseGeneratorTool\n")
    
    try:
        # åˆ›å»ºå·¥å…·å®ä¾‹
        tool = ResponseGeneratorTool()
        
        # æ¨¡æ‹ŸçŸ¥è¯†æ°´å¹³ä¿¡æ¯
        knowledge_level = {
            "subjects_info": {
                "astronomy": {
                    "level": "beginner",
                    "detailed_description": "åˆšå¼€å§‹å­¦ä¹ å¤©æ–‡å­¦ï¼Œäº†è§£åŸºæœ¬æ¦‚å¿µ"
                }
            }
        }
        
        # æ¨¡æ‹Ÿæ–‡ä»¶ä¿¡æ¯
        selected_files = [
            {
                "title": "å¤ªé˜³åŸºç¡€çŸ¥è¯†.pdf",
                "content_summary": "ä»‹ç»å¤ªé˜³çš„åŸºæœ¬ç»“æ„ã€æ¸©åº¦å’Œèƒ½é‡æ¥æº",
                "topics": ["astronomy", "physics"]
            }
        ]
        
        # ç”Ÿæˆå“åº”
        response = tool.generate_response_stream(
            user_query="å¤ªé˜³çš„å†…éƒ¨ç»“æ„æ˜¯ä»€ä¹ˆï¼Ÿ",
            knowledge_level_info=knowledge_level,
            selected_files=selected_files
        )
        
        print("\nâœ“ æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâœ— æµ‹è¯•å¤±è´¥: {e}")
